{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Avazu\n",
    "\n",
    "**What this notebook does**\n",
    "Interactive, reproducible EDA for the Avazu CTR dataset using **Plotly** with best-practice checks and corrections.  \n",
    "It extends the prior version with: robust categorical casting, train–test schema/drift preview, clustered correlations,\n",
    "date rollups, ANOVA/Kruskal tests, missingness↔target diagnostics with CIs, monotonicity hints, and an imputation strategy inventory.\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "1. [Config & Imports](#config)  \n",
    "2. [Changelog & Corrections](#changelog)  \n",
    "3. [Utilities (logging, memory, stats)](#utils)  \n",
    "4. [Load Data (typed, memory-optimized)](#load)  \n",
    "5. [Data Overview](#overview)  \n",
    "6. [Missingness Analysis](#missingness)  \n",
    "7. [Target-Aware Summary](#targetaware)  \n",
    "8. [Univariate Analysis](#univariate)  \n",
    "9. [Bivariate & Multivariate Analysis](#multivariate)  \n",
    "10. [Time Series Handling](#timeseries)  \n",
    "11. [Train–Test Schema & Drift Preview](#drift)  \n",
    "12. [Date Feature Rollups](#daterollups)  \n",
    "13. [Mixed Tests: ANOVA & Kruskal](#anova)  \n",
    "14. [Missingness ↔ Target Diagnostics](#missxtgt)  \n",
    "15. [Clustered Correlation Heatmap](#clusteredcorr)  \n",
    "16. [Monotonicity Hints vs Time](#monotonicity)  \n",
    "17. [Imputation Strategy Inventory (Not Recommendations)](#imputeinventory)  \n",
    "18. [Feature Quality Checks](#quality)  \n",
    "19. [Appendix (helpers)](#appendix)\n",
    "\n",
    "---\n",
    "\n",
    "### How to interpret this notebook overall\n",
    "- Confirm schema and memory profile; watch for high-cardinality identifiers and degenerate columns.  \n",
    "- Review missingness and co-missingness; examine any association with the target (diagnostic only).  \n",
    "- Inspect univariate distributions; for categorical, look at top-k and rare levels.  \n",
    "- Review bivariate associations (numeric/categorical) and correlation structure (clustered).  \n",
    "- For time-based features, verify continuity, gaps, and periodic rollups.  \n",
    "- No subjective interpretation or modeling guidance is provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import leaves_list, linkage\n",
    "from scipy.stats import chi2_contingency, f_oneway, kruskal, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Paths ----------\n",
    "DATA_DIR = Path(\"../data/raw/avazu\")\n",
    "TRAIN_FILE = DATA_DIR / \"train.gz\"\n",
    "TEST_FILE = DATA_DIR / \"test.gz\"\n",
    "\n",
    "# ---------- Core Columns (Avazu) ----------\n",
    "ID_COL = \"id\"\n",
    "TARGET_COL = \"click\"\n",
    "DATETIME_COL = \"hour\"\n",
    "DATETIME_FORMAT = \"%y%m%d%H\"\n",
    "\n",
    "# ---------- Reading & Sampling ----------\n",
    "SAMPLE_FRAC = 0.05\n",
    "RANDOM_STATE = 42\n",
    "LOW_MEMORY = True\n",
    "\n",
    "# ---------- Casting Controls ----------\n",
    "DTYPE_COLS = {\n",
    "    \"id\": np.int64,\n",
    "    \"click\": np.int8,\n",
    "    \"hour\": np.int64,\n",
    "    \"C1\": np.int32,\n",
    "    \"banner_pos\": np.int8,\n",
    "    \"site_id\": \"category\",\n",
    "    \"site_domain\": \"category\",\n",
    "    \"site_category\": \"category\",\n",
    "    \"app_id\": \"category\",\n",
    "    \"app_domain\": \"category\",\n",
    "    \"app_category\": \"category\",\n",
    "    \"device_id\": \"category\",\n",
    "    \"device_ip\": \"category\",\n",
    "    \"device_model\": \"category\",\n",
    "    \"device_type\": np.int8,\n",
    "    \"device_conn_type\": np.int8,\n",
    "    \"C14\": np.int32,\n",
    "    \"C15\": np.int32,\n",
    "    \"C16\": np.int32,\n",
    "    \"C17\": np.int32,\n",
    "    \"C18\": np.int32,\n",
    "    \"C19\": np.int32,\n",
    "    \"C20\": np.int32,\n",
    "    \"C21\": np.int32,\n",
    "}\n",
    "ALL_COLS = list(DTYPE_COLS.keys())\n",
    "\n",
    "# ---------- Plotting & Reporting ----------\n",
    "MAX_UNIVAR_PLOTS = 12\n",
    "TOP_K_CATEGORIES = 20\n",
    "RARE_LEVEL_THRESHOLD = 0.01\n",
    "HEAVY_PLOT_SAMPLE = 500_000\n",
    "CARDINALITY_BUCKET_LABELS = {\n",
    "    0: \"very_low\",\n",
    "    1: \"low\",\n",
    "    2: \"medium\",\n",
    "    3: \"high\",\n",
    "    4: \"very_high\",\n",
    "}\n",
    "\n",
    "# ---------- Correlations & Stats ----------\n",
    "CORR_METHODS = (\"pearson\", \"spearman\")\n",
    "HIGH_CORR_THRESHOLD = 0.9\n",
    "CRAMERS_V_MAX_CAT_LEVELS = 30\n",
    "THEIL_U_MAX_CAT_LEVELS = 30\n",
    "OTHER_LABEL = \"__OTHER__\"\n",
    "\n",
    "# ---------- Time Series ----------\n",
    "DECOMPOSE_PERIOD = 24\n",
    "ROLLING_WINDOW = 24\n",
    "\n",
    "# ---------- Performance ----------\n",
    "DISPLAY_SAMPLES = 10_000\n",
    "VIF_MAX_NUMERIC = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg: str):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Confirm expected schema, dtypes, and memory usage. Casting is conditional for potentially high-cardinality IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_infer(path, usecols=None):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        usecols=usecols,\n",
    "        dtype=DTYPE_COLS,\n",
    "        low_memory=LOW_MEMORY,\n",
    "        compression=\"infer\",\n",
    "    )\n",
    "\n",
    "\n",
    "def memory_usage_mb(df: pd.DataFrame) -> float:\n",
    "    return df.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "\n",
    "def downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_integer_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "        elif pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_hour_to_datetime(series: pd.Series) -> pd.Series:\n",
    "    ser = series.astype(str).str.zfill(8)\n",
    "    return pd.to_datetime(ser, format=DATETIME_FORMAT, errors=\"coerce\", utc=True)\n",
    "\n",
    "\n",
    "log(\"Reading Avazu data...\")\n",
    "\n",
    "df = read_csv_infer(TRAIN_FILE, usecols=ALL_COLS)\n",
    "log(f\"Train read: shape={df.shape}, mem={memory_usage_mb(df):.2f} MB\")\n",
    "\n",
    "df_test = read_csv_infer(TEST_FILE, usecols=[c for c in ALL_COLS if c != TARGET_COL])\n",
    "log(f\"Test read: shape={df_test.shape}, mem={memory_usage_mb(df_test):.2f} MB\")\n",
    "\n",
    "df[DATETIME_COL] = parse_hour_to_datetime(df[DATETIME_COL])\n",
    "\n",
    "df = downcast_numeric(df)\n",
    "log(f\"After downcast: mem={memory_usage_mb(df):.2f} MB\")\n",
    "\n",
    "df = df.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "log(\n",
    "    f\"Sampled df to fraction={SAMPLE_FRAC}: shape={df.shape}, mem={memory_usage_mb(df):.2f} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Data Overview\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Verify row/column counts, dtypes, memory footprint; identify duplicates, constant & near-zero-variance columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_zero_variance(df: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        vc = df[c].value_counts(dropna=False)\n",
    "        top_share = (vc.iloc[0] / len(df)) if len(vc) else np.nan\n",
    "        rows.append(\n",
    "            {\n",
    "                \"column\": c,\n",
    "                \"top_value_share\": float(top_share),\n",
    "                \"near_zero_var\": bool(top_share >= threshold),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"top_value_share\", ascending=False)\n",
    "\n",
    "\n",
    "log(\"Overview...\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Dtypes:\\n\", df.dtypes)\n",
    "\n",
    "print(\"Head:\")\n",
    "display(df.head(5))\n",
    "print(\"Tail:\")\n",
    "display(df.tail(5))\n",
    "\n",
    "const_cols = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]\n",
    "print(\"Constant columns:\", const_cols)\n",
    "\n",
    "nzv = near_zero_variance(df)\n",
    "display(nzv.sort_values(\"near_zero_var\", ascending=False).head(20))\n",
    "\n",
    "n_dup = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {n_dup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Missingness Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Inspect high-missing columns, co-missingness patterns, and row-level missingness. Patterns may hint MNAR (not assessed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Missingness analysis...\")\n",
    "\n",
    "miss_pct = df.isna().mean().sort_values(ascending=False) * 100.0\n",
    "miss_tbl = pd.DataFrame({\"column\": miss_pct.index, \"missing_%\": miss_pct.values})\n",
    "display(miss_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Nullish Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**: ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nullish analysis...\")\n",
    "\n",
    "null_pct = df.isnull().mean().sort_values(ascending=False) * 100.0\n",
    "null_tbl = pd.DataFrame({\"column\": null_pct.index, \"nullish_%\": null_pct.values})\n",
    "display(null_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Target-Aware Summary (Classification)\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Inspect class balance, trivial leakage checks (perfect predictors), and preview splits (random/time-based) without modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_perfect_predictor(x: pd.Series, y: pd.Series) -> bool:\n",
    "    tmp = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "    g = tmp.groupby(\"x\", observed=False)[\"y\"]\n",
    "\n",
    "    if (g.nunique() > 1).any():\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "log(\"Target-aware summary...\")\n",
    "\n",
    "tgt = df[TARGET_COL]\n",
    "print(\"Target dtype:\", tgt.dtype)\n",
    "print(\"Target value counts:\")\n",
    "display(\n",
    "    tgt.value_counts(dropna=False)\n",
    "    .to_frame(\"count\")\n",
    "    .assign(share=lambda x: x[\"count\"] / len(tgt))\n",
    ")\n",
    "\n",
    "px.histogram(tgt, title=f\"Target Distribution — {TARGET_COL}\", nbins=2).show()\n",
    "\n",
    "perfect_cols = []\n",
    "for c in df.columns:\n",
    "    if c == TARGET_COL:\n",
    "        continue\n",
    "\n",
    "    if is_perfect_predictor(df[c], tgt):\n",
    "        perfect_cols.append(c)\n",
    "\n",
    "print(\"Columns that perfectly predict target (potential leakage):\", perfect_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Univariate Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Numeric: skew & outlier %, hist/box/violin. Categorical: frequency, top-k, rare levels, high-card flags. Text: lengths & tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Univariate analysis...\")\n",
    "\n",
    "num_cols = (\n",
    "    df.select_dtypes(include=[np.number]).drop(columns=[TARGET_COL]).columns.tolist()\n",
    ")\n",
    "cat_cols = df.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns (excl. target):\", len(num_cols))\n",
    "print(\"Categorical-like columns:\", len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_outlier_stats(s: pd.Series) -> dict:\n",
    "    # Skewness\n",
    "    sk = skew(s, bias=False)\n",
    "\n",
    "    # IQR outlier\n",
    "    q1, q3 = np.percentile(s, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    iqr_outlier = ((s < lower) | (s > upper)).mean() * 100.0\n",
    "\n",
    "    # Z-Score outlier\n",
    "    z = np.abs(stats.zscore(s, nan_policy=\"omit\"))\n",
    "    z_outlier = (z > 3).mean() * 100.0\n",
    "\n",
    "    return {\n",
    "        \"skewness\": float(sk),\n",
    "        \"iqr_outlier_pct\": float(iqr_outlier),\n",
    "        \"zscore_outlier_pct\": float(z_outlier),\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_cardinality_bucket(\n",
    "    unique_cnt: int,\n",
    "    unique_rt: float,\n",
    "    a_thresholds: tuple[int],\n",
    "    r_thresholds: tuple[float],\n",
    ") -> str:\n",
    "    def absolute_level(count: int) -> int:\n",
    "        a0, a1, a2, a3 = a_thresholds\n",
    "\n",
    "        if count <= a0:\n",
    "            return 0\n",
    "        if count <= a1:\n",
    "            return 1\n",
    "        if count <= a2:\n",
    "            return 2\n",
    "        if count <= a3:\n",
    "            return 3\n",
    "\n",
    "        return 4\n",
    "\n",
    "    def relative_level(ratio: float) -> int:\n",
    "        r0, r1, r2, r3 = r_thresholds\n",
    "\n",
    "        if ratio < r0:\n",
    "            return 0\n",
    "        if ratio < r1:\n",
    "            return 1\n",
    "        if ratio < r2:\n",
    "            return 2\n",
    "        if ratio < r3:\n",
    "            return 3\n",
    "\n",
    "        return 4\n",
    "\n",
    "    level = max(absolute_level(unique_cnt), relative_level(unique_rt))\n",
    "    return CARDINALITY_BUCKET_LABELS[level]\n",
    "\n",
    "\n",
    "# --- Numeric summaries + plots ---\n",
    "num_report = []\n",
    "for c in num_cols:\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\").astype(float)\n",
    "    out = robust_outlier_stats(s)\n",
    "\n",
    "    num_report.append({\"column\": c, **out})\n",
    "\n",
    "# --- Categorical summaries ---\n",
    "cat_report = []\n",
    "for c in cat_cols:\n",
    "    vc = df[c].value_counts()\n",
    "\n",
    "    topk = vc.head(TOP_K_CATEGORIES).to_frame(\"count\")\n",
    "    topk[\"share\"] = topk[\"count\"] / len(df)\n",
    "\n",
    "    unique_cnt = int(df[c].nunique())\n",
    "    unique_rt = unique_cnt / len(df)\n",
    "\n",
    "    cardinality_bucket = classify_cardinality_bucket(\n",
    "        unique_cnt=unique_cnt,\n",
    "        unique_rt=unique_rt,\n",
    "        a_thresholds=(10, 100, 1_000, 100_000),\n",
    "        r_thresholds=(0.0001, 0.001, 0.01, 0.1),\n",
    "    )\n",
    "\n",
    "    rare_mask = vc / len(df) < RARE_LEVEL_THRESHOLD\n",
    "    rare_levels_share = (vc[rare_mask].sum() / len(df)) if rare_mask.any() else 0.0\n",
    "\n",
    "    px.bar(\n",
    "        topk.reset_index().rename(columns={\"index\": c}),\n",
    "        x=c,\n",
    "        y=\"count\",\n",
    "        title=f\"Top-{TOP_K_CATEGORIES} — {c}\",\n",
    "    ).show()\n",
    "    cat_report.append(\n",
    "        {\n",
    "            \"column\": c,\n",
    "            \"unique\": unique_cnt,\n",
    "            \"cardinality_bucket\": cardinality_bucket,\n",
    "            \"rare_levels_share\": float(rare_levels_share),\n",
    "        }\n",
    "    )\n",
    "\n",
    "display(pd.DataFrame(num_report))\n",
    "display(pd.DataFrame(cat_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Bivariate & Multivariate Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Numeric↔target (box/violin + decile lift with CIs), cat↔target (contingency with χ²/Fisher), num↔num correlations, cat↔cat (Cramér’s V, Theil’s U).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Bivariate & multivariate...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilson_ci(k: float, n: int, z: float) -> tuple[float, float]:\n",
    "    denominator = 1 + z**2 / n\n",
    "    center_adj = k + z**2 / (2 * n)\n",
    "    term = z * ((k * (1 - k)) / n + z**2 / (4 * n**2)) ** 0.5\n",
    "\n",
    "    ci_low = ((center_adj - term) / denominator).fillna(0)\n",
    "    ci_high = ((center_adj + term) / denominator).fillna(1)\n",
    "    return ci_low, ci_high\n",
    "\n",
    "\n",
    "# Numeric ↔ Target\n",
    "base_rate = df[TARGET_COL].mean()\n",
    "for c in num_cols:\n",
    "    bin_labels = pd.qcut(df[c], q=10, duplicates=\"drop\").astype(str)\n",
    "    bins = pd.qcut(df[c], q=10, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    grp = (\n",
    "        df.groupby(bins)[TARGET_COL]\n",
    "        .agg([\"sum\", \"count\"])\n",
    "        .rename(columns={\"sum\": \"clicks\", \"count\": \"n\"})\n",
    "    )\n",
    "    grp[\"ctr\"] = grp[\"clicks\"] / grp[\"n\"]\n",
    "    grp[\"lift\"] = grp[\"ctr\"] / base_rate\n",
    "\n",
    "    z = stats.norm.ppf(1 - 0.05 / 2)\n",
    "    ci_low, ci_high = wilson_ci(grp[\"ctr\"], grp[\"n\"], z)\n",
    "    grp[\"ci_low\"] = ci_low\n",
    "    grp[\"ci_high\"] = ci_high\n",
    "    display(grp)\n",
    "\n",
    "    x_axis = list(range(len(grp)))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x_axis, y=grp[\"ctr\"], mode=\"lines+markers\", name=\"CTR\"))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_axis + x_axis[::-1],\n",
    "            y=list(grp[\"ci_high\"]) + list(grp[\"ci_low\"])[::-1],\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(0,100,80,0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "            name=\"95% CI\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=f\"CTR by {c} Quantiles (with 95% Wilson CI)\",\n",
    "        xaxis_title=f\"{c} Quantile Bin (ordered low to high)\",\n",
    "        yaxis_title=\"Click-Through Rate (CTR)\",\n",
    "        xaxis=dict(tickvals=x_axis, ticktext=grp.index),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical ↔ Target\n",
    "base_rate = float(df[TARGET_COL].mean())\n",
    "for c in cat_cols:\n",
    "    s = df[c]\n",
    "\n",
    "    vc = s.value_counts()\n",
    "    keep_idx = vc.nlargest(TOP_K_CATEGORIES).index\n",
    "    keep_idx = vc[vc >= 100_000].index.union(keep_idx)\n",
    "    if len(keep_idx) == 0 and len(vc) > 0:\n",
    "        keep_idx = vc.nlargest(min(5, len(vc))).index\n",
    "\n",
    "    if OTHER_LABEL not in s.cat.categories:\n",
    "        s = s.cat.add_categories([OTHER_LABEL])\n",
    "\n",
    "    collapsed = s.where(s.isin(keep_idx), OTHER_LABEL)\n",
    "\n",
    "    tmp = pd.DataFrame({c: collapsed, TARGET_COL: df[TARGET_COL]})\n",
    "    counts = pd.crosstab(tmp[c], tmp[TARGET_COL])\n",
    "\n",
    "    col_idx = pd.Index(sorted(tmp[TARGET_COL].unique()), name=TARGET_COL)\n",
    "    counts = counts.reindex(columns=col_idx, fill_value=0)\n",
    "\n",
    "    chi2, p, dof, exp = chi2_contingency(counts.values)\n",
    "    exp_min = float(exp.min())\n",
    "    log(f\"[{c}] χ²={chi2:.3f}, dof={dof}, p={p:.3g}, min_expected={exp_min:.2f}\")\n",
    "\n",
    "    rate_tbl = (\n",
    "        tmp.groupby(c, observed=True)[TARGET_COL]\n",
    "        .agg(ctr=\"mean\", count=\"size\")\n",
    "        .assign(lift=lambda t: t[\"ctr\"] / base_rate if base_rate > 0 else float(\"nan\"))\n",
    "        .sort_values([\"count\", \"ctr\"], ascending=[False, False])\n",
    "    )\n",
    "    display(rate_tbl)\n",
    "\n",
    "    plot_df = rate_tbl.nlargest(TOP_K_CATEGORIES, \"count\").sort_values(\n",
    "        \"ctr\", ascending=False\n",
    "    )\n",
    "    fig = px.bar(\n",
    "        plot_df.reset_index(),\n",
    "        x=c,\n",
    "        y=\"ctr\",\n",
    "        title=f\"Target rate by {c} (top {len(plot_df)}; tail→{OTHER_LABEL})\",\n",
    "    )\n",
    "    fig.update_layout(xaxis={\"categoryorder\": \"total descending\", \"tickangle\": -45})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric ↔ Numeric\n",
    "for method in CORR_METHODS:\n",
    "    num_df = df[num_cols]\n",
    "    corr = num_df.corr(method=method)\n",
    "\n",
    "    px.imshow(\n",
    "        corr, aspect=\"auto\", text_auto=\".2f\", title=f\"{method.title()} Correlations\"\n",
    "    ).show()\n",
    "\n",
    "    pairs = []\n",
    "    cols = corr.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            v = corr.iloc[i, j]\n",
    "\n",
    "            if abs(v) >= HIGH_CORR_THRESHOLD:\n",
    "                pairs.append((cols[i], cols[j], float(v)))\n",
    "\n",
    "    if pairs:\n",
    "        display(\n",
    "            pd.DataFrame(pairs, columns=[\"col1\", \"col2\", \"corr\"]).sort_values(\n",
    "                \"corr\", ascending=False\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v_corrected(confusion: pd.DataFrame) -> float:\n",
    "    chi2 = chi2_contingency(confusion)[0]\n",
    "    n = confusion.values.sum()\n",
    "\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    denom = min((kcorr - 1), (rcorr - 1))\n",
    "\n",
    "    return float(np.sqrt(phi2corr / denom))\n",
    "\n",
    "\n",
    "def theils_u(x: pd.Series, y: pd.Series) -> float:\n",
    "    def entropy(s: pd.Series) -> float:\n",
    "        p = s.value_counts(normalize=True, dropna=False)\n",
    "        return float(stats.entropy(p, base=2))\n",
    "\n",
    "    s_xy = pd.crosstab(x, y)\n",
    "    pxy = s_xy / s_xy.values.sum()\n",
    "    py = pxy.sum(axis=0)\n",
    "\n",
    "    h_x = entropy(x)\n",
    "    h_x_given_y = 0.0\n",
    "\n",
    "    for yv in py.index:\n",
    "        p_y = py.loc[yv]\n",
    "\n",
    "        if p_y > 0:\n",
    "            p_x_given_y = pxy.loc[:, yv] / p_y\n",
    "            h_x_given_y += p_y * stats.entropy(p_x_given_y.fillna(0), base=2)\n",
    "\n",
    "    return float((h_x - h_x_given_y) / h_x)\n",
    "\n",
    "\n",
    "# Categorical ↔ Categorical\n",
    "small_cats = [c for c in cat_cols if df[c].nunique() <= 100_000]\n",
    "pairs, pairs_u = [], []\n",
    "\n",
    "for i in range(len(small_cats)):\n",
    "    for j in range(i + 1, len(small_cats)):\n",
    "        a, b = small_cats[i], small_cats[j]\n",
    "        ct = pd.crosstab(df[a], df[b])\n",
    "\n",
    "        if ct.shape[0] >= 2 and ct.shape[1] >= 2:\n",
    "            v = cramers_v_corrected(ct)\n",
    "            pairs.append((a, b, float(v)))\n",
    "\n",
    "        if (\n",
    "            df[a].nunique() <= 50_000\n",
    "            and df[b].nunique() <= 50_000\n",
    "        ):\n",
    "            # Symmetrize by averaging U(X|Y) and U(Y|X)\n",
    "            uab = theils_u(df[a], df[b])\n",
    "            uba = theils_u(df[b], df[a])\n",
    "            pairs_u.append((a, b, float((uab + uba) / 2)))\n",
    "\n",
    "if pairs:\n",
    "    display(\n",
    "        pd.DataFrame(pairs, columns=[\"cat1\", \"cat2\", \"cramers_v\"]).sort_values(\n",
    "            \"cramers_v\", ascending=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "if pairs_u:\n",
    "    display(\n",
    "        pd.DataFrame(pairs_u, columns=[\"cat1\", \"cat2\", \"theils_u_avg\"]).sort_values(\n",
    "            \"theils_u_avg\", ascending=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Time Series Handling\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Inspect continuity, decomposition, stationarity, rolling stats. No forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Time series diagnostics on aggregated CTR by hour or counts...\")\n",
    "\n",
    "ts = (\n",
    "    df.set_index(DATETIME_COL)\n",
    "    .groupby(pd.Grouper(freq=\"h\"))[TARGET_COL]\n",
    "    .mean()\n",
    "    .rename(\"rate\")\n",
    ")\n",
    "y_label = \"CTR\"\n",
    "\n",
    "display(ts.describe())\n",
    "\n",
    "full_idx = pd.date_range(ts.index.min(), ts.index.max(), freq=\"h\", tz=\"UTC\")\n",
    "missing_periods = full_idx.difference(ts.index)\n",
    "print(\"Missing hourly periods:\", len(missing_periods))\n",
    "print(\"Timezone:\", ts.index.tz)\n",
    "\n",
    "px.line(\n",
    "    ts.reset_index(), x=DATETIME_COL, y=ts.name, title=\"Observed time series\"\n",
    ").update_layout(yaxis_title=y_label).show()\n",
    "\n",
    "res = seasonal_decompose(\n",
    "    ts,\n",
    "    period=DECOMPOSE_PERIOD,\n",
    "    model=\"additive\",\n",
    "    two_sided=False,\n",
    "    extrapolate_trend=\"freq\",\n",
    ")\n",
    "for y, part_name, part_series in [\n",
    "    (\"trend\", \"Trend\", res.trend),\n",
    "    (\"seasonal\", \"Seasonality\", res.seasonal),\n",
    "    (\"resid\", \"Residual\", res.resid),\n",
    "]:\n",
    "    px.line(part_series.reset_index(), x=DATETIME_COL, y=y, title=part_name).show()\n",
    "\n",
    "series = ts.values\n",
    "result = adfuller(series, autolag=\"AIC\")\n",
    "print(\n",
    "    \"ADF:\",\n",
    "    {\"stat\": result[0], \"pvalue\": result[1], \"lags\": result[2], \"nobs\": result[3]},\n",
    ")\n",
    "\n",
    "rmean = ts.rolling(ROLLING_WINDOW).mean().rename(\"rolling_mean\")\n",
    "rstd = ts.rolling(ROLLING_WINDOW).std().rename(\"rolling_std\")\n",
    "tmp = pd.concat([ts, rmean, rstd], axis=1).reset_index()\n",
    "px.line(\n",
    "    tmp,\n",
    "    x=DATETIME_COL,\n",
    "    y=[ts.name, \"rolling_mean\", \"rolling_std\"],\n",
    "    title=f\"Rolling mean/std (window={ROLLING_WINDOW})\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Date Feature Rollups\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Roll-ups by hour-of-day, day-of-week; DOW×HOD heatmap (CTR or counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.copy()\n",
    "tmp[\"hod\"] = tmp[DATETIME_COL].dt.hour\n",
    "tmp[\"dow\"] = tmp[DATETIME_COL].dt.dayofweek\n",
    "\n",
    "hod = tmp.groupby(\"hod\")[TARGET_COL].mean().reset_index()\n",
    "dow = tmp.groupby(\"dow\")[TARGET_COL].mean().reset_index()\n",
    "mat = tmp.groupby([\"dow\", \"hod\"])[TARGET_COL].mean().unstack(0)\n",
    "y_label = \"CTR\"\n",
    "\n",
    "px.bar(hod, x=\"hod\", y=hod.columns[1], title=f\"Hour-of-day {y_label}\").show()\n",
    "px.bar(dow, x=\"dow\", y=dow.columns[1], title=f\"Day-of-week {y_label}\").show()\n",
    "px.imshow(\n",
    "    mat.transpose(),\n",
    "    aspect=\"auto\",\n",
    "    text_auto=\".2f\",\n",
    "    title=f\"DOW x HOD heatmap ({y_label})\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Train–Test Schema & Drift Preview\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Compare basic schema, dtype consistency, and category overlaps; report unseen categories (test vs train).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Train-Test schema & drift preview...\")\n",
    "\n",
    "common_cols = [c for c in df.columns if c in df_test.columns]\n",
    "dtypes_cmp = pd.DataFrame(\n",
    "    {\n",
    "        \"train_dtype\": df[common_cols].dtypes.astype(str),\n",
    "        \"test_dtype\": df_test[common_cols].dtypes.astype(str),\n",
    "    }\n",
    ")\n",
    "dtypes_cmp[\"match\"] = dtypes_cmp[\"train_dtype\"] == dtypes_cmp[\"test_dtype\"]\n",
    "display(dtypes_cmp)\n",
    "\n",
    "drift_rows = []\n",
    "for c in common_cols:\n",
    "    if str(df[c].dtype) in (\"category\", \"object\", \"string\"):\n",
    "        tr_levels = set(df[c].unique())\n",
    "        te_levels = set(df_test[c].unique())\n",
    "\n",
    "        unseen_in_test = len(te_levels - tr_levels)\n",
    "        unseen_rate = unseen_in_test / max(len(te_levels), 1)\n",
    "        drift_rows.append(\n",
    "            {\n",
    "                \"column\": c,\n",
    "                \"train_levels\": len(tr_levels),\n",
    "                \"test_levels\": len(te_levels),\n",
    "                \"unseen_in_test\": unseen_in_test,\n",
    "                \"unseen_rate\": float(unseen_rate),\n",
    "            }\n",
    "        )\n",
    "\n",
    "if drift_rows:\n",
    "    display(\n",
    "        pd.DataFrame(drift_rows).sort_values(\"unseen_rate\", ascending=False).head(30)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Mixed Tests: ANOVA & Kruskal (stats only)\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** For selected numeric vs categorical pairs (top-k categories), report test statistics only. No interpretation/conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in num_cols:\n",
    "    for cat in cat_cols:\n",
    "        vc = df[cat].value_counts().head(TOP_K_CATEGORIES).index\n",
    "        gg = [df.loc[df[cat]==lvl, num] for lvl in vc]\n",
    "        gg = [g for g in gg if len(g) > 1]\n",
    "\n",
    "        if len(gg) < 2:\n",
    "            continue\n",
    "\n",
    "        fstat, fp = f_oneway(*gg)\n",
    "        kstat, kp = kruskal(*gg)\n",
    "        log(f\"{num} ~ {cat} | ANOVA F={fstat:.3g}, p={fp:.3g} | Kruskal H={kstat:.3g}, p={kp:.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Clustered Correlation Heatmap\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Hierarchically cluster the correlation matrix to reveal blocks of related features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df[num_cols].drop(columns=[ID_COL])\n",
    "\n",
    "corr = num_df.corr(method=\"pearson\").fillna(0)\n",
    "dist = 1 - corr.abs()\n",
    "Z = linkage(dist, method=\"average\")\n",
    "order = leaves_list(Z)\n",
    "corr_ord = corr.iloc[order, :].iloc[:, order]\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_ord, aspect=\"auto\", text_auto=\".2f\", title=\"Correlation Heatmap\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Monotonicity Hints vs Time\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Spearman correlation between features and chronological order (monotonic trends).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df[DATETIME_COL].rank(method=\"first\").astype(float)\n",
    "rows = []\n",
    "\n",
    "for c in num_cols + cat_cols:\n",
    "    s = df[c]\n",
    "    rho, p = stats.spearmanr(order.loc[s.index], s, nan_policy=\"omit\")\n",
    "    rows.append(\n",
    "        {\n",
    "            \"column\": c,\n",
    "            \"spearman_rho_vs_time\": float(rho),\n",
    "            \"p_value\": float(p),\n",
    "        }\n",
    "    )\n",
    "\n",
    "if rows:\n",
    "    display(pd.DataFrame(rows).sort_values(\"spearman_rho_vs_time\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Feature Quality Checks\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:** Recap missingness, cardinality, multicollinearity (VIF), constant columns, and potential leakage flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cardinality(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    n = len(df)\n",
    "    rows = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        u = df[c].nunique(dropna=True)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"column\": c,\n",
    "                \"dtype\": str(df[c].dtype),\n",
    "                \"unique_count\": int(u),\n",
    "                \"unique_ratio\": float(u / max(n, 1)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"unique_count\", ascending=False)\n",
    "\n",
    "\n",
    "log(\"Feature quality checks...\")\n",
    "\n",
    "card = summarize_cardinality(df)\n",
    "display(card.head(20))\n",
    "\n",
    "num_cols_for_vif = df[num_cols].columns.to_list()\n",
    "\n",
    "X = df[num_cols_for_vif].astype(float)\n",
    "X = X.loc[:, X.std() > 0]\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "vifs = []\n",
    "for i, c in enumerate(X.columns):\n",
    "    v = variance_inflation_factor(X.values, i)\n",
    "    vifs.append(\n",
    "        {\n",
    "            \"column\": c,\n",
    "            \"VIF\": float(v),\n",
    "        }\n",
    "    )\n",
    "\n",
    "display(pd.DataFrame(vifs).sort_values(\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "click-through-learn-j_p1wCCt-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
