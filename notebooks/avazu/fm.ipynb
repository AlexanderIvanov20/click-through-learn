{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030f9502",
   "metadata": {},
   "source": [
    "## Logistic Regression + Factorization Machines (FM/FFM) for Avazu CTR\n",
    "\n",
    "---\n",
    "\n",
    "**Goal:** Train a strong CTR model on the Avazu dataset using:\n",
    "- Baseline **Logistic Regression** (sparse one-hot / hashing features)\n",
    "- **Factorization Machines (FM)** with **logistic loss** via `fastFM`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18f522",
   "metadata": {},
   "source": [
    "#### Imports & Config\n",
    "\n",
    "---\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87926c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Self\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils.validation import check_is_fitted, check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e1baf",
   "metadata": {},
   "source": [
    "#### Paths & Avazu schema\n",
    "\n",
    "---\n",
    "\n",
    "- Download Avazu `\"train\"` CSV (from Kaggle) and set `AVAZU_PATH` to the file.\n",
    "- We will use **day-based split** by parsing the `hour` column (format `YYYYMMDDHH`) and selecting the **last day as validation** to avoid temporal leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50dcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/raw/avazu\")\n",
    "TRAIN_FILE = DATA_DIR / \"train.gz\"\n",
    "TEST_FILE = DATA_DIR / \"test.gz\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_FRAC = 0.05\n",
    "\n",
    "DTYPE_COLS = {\n",
    "    \"id\": np.int64,\n",
    "    \"click\": np.int8,\n",
    "    \"hour\": np.int64,\n",
    "    \"C1\": np.int32,\n",
    "    \"banner_pos\": np.int8,\n",
    "    \"site_id\": \"category\",\n",
    "    \"site_domain\": \"category\",\n",
    "    \"site_category\": \"category\",\n",
    "    \"app_id\": \"category\",\n",
    "    \"app_domain\": \"category\",\n",
    "    \"app_category\": \"category\",\n",
    "    \"device_id\": \"category\",\n",
    "    \"device_ip\": \"category\",\n",
    "    \"device_model\": \"category\",\n",
    "    \"device_type\": np.int8,\n",
    "    \"device_conn_type\": np.int8,\n",
    "    \"C14\": np.int32,\n",
    "    \"C15\": np.int32,\n",
    "    \"C16\": np.int32,\n",
    "    \"C17\": np.int32,\n",
    "    \"C18\": np.int32,\n",
    "    \"C19\": np.int32,\n",
    "    \"C20\": np.int32,\n",
    "    \"C21\": np.int32,\n",
    "}\n",
    "\n",
    "# Columns to treat as categorical (strings). `hour` will be parsed into day/hour.\n",
    "CAT_COLS = [\n",
    "    \"banner_pos\",\n",
    "    \"site_id\",\n",
    "    \"site_domain\",\n",
    "    \"site_category\",\n",
    "    \"app_id\",\n",
    "    \"app_domain\",\n",
    "    \"app_category\",\n",
    "    \"device_id\",\n",
    "    \"device_ip\",\n",
    "    \"device_model\",\n",
    "    \"device_type\",\n",
    "    \"device_conn_type\",\n",
    "    \"C14\",\n",
    "    \"C15\",\n",
    "    \"C16\",\n",
    "    \"C18\",\n",
    "    \"C19\",\n",
    "    \"C20\",\n",
    "    \"C21\",\n",
    "]\n",
    "TARGET_COL = \"click\"\n",
    "\n",
    "ID_COL = \"id\"\n",
    "TARGET_COL = \"click\"\n",
    "DATETIME_COL = \"hour\"\n",
    "DATETIME_FORMAT = \"%y%m%d%H\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "HASH_N_FEATURES = 2**22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d4aa7",
   "metadata": {},
   "source": [
    "#### Utilities: day-based split & chunked reader\n",
    "\n",
    "---\n",
    "\n",
    "We split **by last day** (validation) to mimic online production and avoid target leakage across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb0461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train read: shape=(40428967, 24)\n",
      "Sampled df to fraction=0.05: shape=(2021448, 24)\n"
     ]
    }
   ],
   "source": [
    "def read_csv_infer(path: Path, usecols: list[str] = None) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, usecols=usecols, dtype=DTYPE_COLS, compression=\"infer\")\n",
    "\n",
    "\n",
    "df = read_csv_infer(TRAIN_FILE)\n",
    "print(f\"Train read: shape={df.shape}\")\n",
    "\n",
    "df = df.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "print(f\"Sampled df to fraction={SAMPLE_FRAC}: shape={df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233b173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = df[DATETIME_COL].astype(str).str.zfill(8)\n",
    "    ts = pd.to_datetime(s, format=DATETIME_FORMAT, errors=\"coerce\", utc=True)\n",
    "    return df.assign(hod=ts.dt.hour, dow=ts.dt.day_of_week)\n",
    "\n",
    "\n",
    "df = add_time_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f861ff",
   "metadata": {},
   "source": [
    "#### Feature engineering for FM and Logistic Regression (hashing)\n",
    "\n",
    "---\n",
    "\n",
    "We will use a **hashing trick** to map `field=value` pairs to a fixed-dimensional sparse space.\n",
    "- This is memory-friendly and scales to Avazu.\n",
    "- FM will model pairwise interactions internally; Logistic Regression will not (baseline).\n",
    "\n",
    "> You can tune `N_FEATS` (e.g., `2**24` or `2**22`) depending on your RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5535c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_DROP = [\n",
    "    ID_COL,\n",
    "    TARGET_COL,\n",
    "    DATETIME_COL,\n",
    "    \"C1\",\n",
    "    \"C17\",\n",
    "]\n",
    "\n",
    "X = df.drop(columns=COLS_TO_DROP, axis=1)\n",
    "y = df[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a2282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_dicts(X: pd.DataFrame) -> list[dict[str, int]]:\n",
    "    return [\n",
    "        {f\"{c}={getattr(row, c)}\": 1 for c in list(X.columns)}\n",
    "        for row in X.itertuples(index=False)\n",
    "    ]\n",
    "\n",
    "\n",
    "hashing_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"cat_pairs\",\n",
    "            FunctionTransformer(func=make_cat_dicts, validate=False),\n",
    "        ),\n",
    "        (\n",
    "            \"cat_hash\",\n",
    "            FeatureHasher(n_features=HASH_N_FEATURES, alternate_sign=False),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cats\", hashing_pipeline, CAT_COLS),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0,  # keep sparse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007c92d",
   "metadata": {},
   "source": [
    "#### Baseline: Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "A strong, well-regularized baseline using sparse hashed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b88e857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: X_train.shape=(1617158, 21)\n",
      "y train: y_train.shape=(1617158,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"X train: {X_train.shape=}\")\n",
    "print(f\"y train: {y_train.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d498bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1\n",
      "Epoch 2, change: 0.19851891\n",
      "Epoch 3, change: 0.11275568\n",
      "Epoch 4, change: 0.075374844\n",
      "Epoch 5, change: 0.056062811\n",
      "Epoch 6, change: 0.049849831\n",
      "Epoch 7, change: 0.049375518\n",
      "Epoch 8, change: 0.034196703\n",
      "Epoch 9, change: 0.034028242\n",
      "Epoch 10, change: 0.030705083\n",
      "Epoch 11, change: 0.023626997\n",
      "Epoch 12, change: 0.031298328\n",
      "Epoch 13, change: 0.016373771\n",
      "Epoch 14, change: 0.015072256\n",
      "Epoch 15, change: 0.014034818\n",
      "Epoch 16, change: 0.012993055\n",
      "Epoch 17, change: 0.012024488\n",
      "Epoch 18, change: 0.011132935\n",
      "Epoch 19, change: 0.0093937501\n",
      "Epoch 20, change: 0.0090100443\n",
      "Epoch 21, change: 0.007372683\n",
      "Epoch 22, change: 0.0069090339\n",
      "Epoch 23, change: 0.006428534\n",
      "Epoch 24, change: 0.0058763374\n",
      "Epoch 25, change: 0.0051660198\n",
      "Epoch 26, change: 0.0048409828\n",
      "Epoch 27, change: 0.0044553124\n",
      "Epoch 28, change: 0.0041379696\n",
      "Epoch 29, change: 0.0038310991\n",
      "Epoch 30, change: 0.0034218334\n",
      "Epoch 31, change: 0.0031736996\n",
      "Epoch 32, change: 0.0029503283\n",
      "Epoch 33, change: 0.0026545058\n",
      "Epoch 34, change: 0.0024890907\n",
      "Epoch 35, change: 0.002260872\n",
      "Epoch 36, change: 0.0020957498\n",
      "Epoch 37, change: 0.0019405513\n",
      "Epoch 38, change: 0.0017419319\n",
      "Epoch 39, change: 0.0016283973\n",
      "Epoch 40, change: 0.0014712639\n",
      "Epoch 41, change: 0.00135058\n",
      "Epoch 42, change: 0.001248834\n",
      "Epoch 43, change: 0.0011507735\n",
      "Epoch 44, change: 0.0010684801\n",
      "Epoch 45, change: 0.00095794164\n",
      "Epoch 46, change: 0.00088249541\n",
      "Epoch 47, change: 0.00081625396\n",
      "Epoch 48, change: 0.00074649528\n",
      "Epoch 49, change: 0.00069065185\n",
      "Epoch 50, change: 0.00063159498\n",
      "Epoch 51, change: 0.00058679147\n",
      "Epoch 52, change: 0.00054334269\n",
      "Epoch 53, change: 0.00048504859\n",
      "Epoch 54, change: 0.00045314897\n",
      "Epoch 55, change: 0.00041959696\n",
      "Epoch 56, change: 0.00037260213\n",
      "Epoch 57, change: 0.00034677351\n",
      "Epoch 58, change: 0.00032341071\n",
      "Epoch 59, change: 0.00029946503\n",
      "Epoch 60, change: 0.00027729176\n",
      "Epoch 61, change: 0.00023794888\n",
      "Epoch 62, change: 0.0002294445\n",
      "Epoch 63, change: 0.00020555727\n",
      "Epoch 64, change: 0.00018997476\n",
      "Epoch 65, change: 0.00017457488\n",
      "Epoch 66, change: 0.00016047503\n",
      "Epoch 67, change: 0.00014904575\n",
      "Epoch 68, change: 0.00013549926\n",
      "Epoch 69, change: 0.00012539643\n",
      "Epoch 70, change: 0.0001161979\n",
      "Epoch 71, change: 0.00010759458\n",
      "convergence after 72 epochs took 77 seconds\n",
      "LogLoss (LR): 0.39924653666078597\n",
      "ROC-AUC (LR): 0.7456562551722086\n",
      "PR-AUC  (LR): 0.36890332432145284\n",
      "Accuracy  (LR): 0.8329219124885602\n"
     ]
    }
   ],
   "source": [
    "# Baseline logistic regression (SAG/SAGA handle sparse well)\n",
    "# Tip: On very high-dimensional hashing, 'saga' with l1 or elasticnet can be strong.\n",
    "lr = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    # class_weight=\"balanced\",  # negatively affects the log loss\n",
    "    random_state=RANDOM_STATE,\n",
    "    solver=\"saga\",\n",
    "    max_iter=1000,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "lr_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"features\", featurizer),\n",
    "        (\"clf\", lr),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_proba = lr_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"LogLoss (LR):\", log_loss(y_test, y_pred_proba))\n",
    "print(\"ROC-AUC (LR):\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"PR-AUC  (LR):\", average_precision_score(y_test, y_pred_proba))\n",
    "print(\"Accuracy  (LR):\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cac4ee",
   "metadata": {},
   "source": [
    "#### Factorization Machines (FM) — logistic loss via `fastFM`\n",
    "\n",
    "---\n",
    "\n",
    "- We reuse the same hashed one-hot features (no manual cross-features; FM learns them).\n",
    "- `fastFM.sgd.FMClassification` expects labels in **{-1, +1}** for logit classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf41e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rank: int = 16,\n",
    "        n_epochs: int = 10,\n",
    "        learning_rate: float = 0.05,\n",
    "        l2_w: float = 1e-6,\n",
    "        l2_V: float = 1e-6,\n",
    "        init_stdev: float = 0.1,\n",
    "        fit_intercept: bool = True,\n",
    "        shuffle: bool = True,\n",
    "        random_state: int = RANDOM_STATE,\n",
    "        verbose: int = 1,\n",
    "    ) -> None:\n",
    "        self.rank = rank\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_w = l2_w\n",
    "        self.l2_V = l2_V\n",
    "        self.init_stdev = init_stdev\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _init_params(self, n_features: int, rng: np.random.RandomState) -> None:\n",
    "        self.w0_ = 0.0\n",
    "        self.w_ = np.zeros(n_features, dtype=np.float64)\n",
    "        self.V_ = rng.normal(0.0, self.init_stdev, size=(n_features, self.rank)).astype(\n",
    "            np.float64\n",
    "        )\n",
    "\n",
    "    def _decision_function_row(self, indices: np.ndarray, data: np.ndarray) -> float:\n",
    "        eta = self.w0_ if self.fit_intercept else 0.0\n",
    "        if indices.size == 0:\n",
    "            return float(eta)\n",
    "\n",
    "        # Linear term\n",
    "        eta += np.dot(self.w_[indices], data)\n",
    "\n",
    "        # Efficient pairwise interactions\n",
    "        V_slice = self.V_[indices, :]  # (nnz, k)\n",
    "        p = V_slice.T @ data  # (k,)\n",
    "        sum_vx_sq = np.sum((V_slice * V_slice) * (data[:, None] ** 2), axis=0)  # (k,)\n",
    "        eta += 0.5 * (np.sum(p * p) - np.sum(sum_vx_sq))\n",
    "        return float(eta)\n",
    "\n",
    "    def decision_function(self, X: sp.csr_matrix) -> np.ndarray:\n",
    "        check_is_fitted(self, attributes=[\"w_\", \"V_\"])\n",
    "\n",
    "        if not sp.isspmatrix_csr(X):\n",
    "            X = sp.csr_matrix(X)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        out = np.empty(n_samples, dtype=np.float64)\n",
    "        for r in range(n_samples):\n",
    "            start, end = X.indptr[r], X.indptr[r + 1]\n",
    "            out[r] = self._decision_function_row(\n",
    "                X.indices[start:end], X.data[start:end]\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def predict_proba(self, X: sp.csr_matrix) -> np.ndarray:\n",
    "        eta = self.decision_function(X)\n",
    "        p1 = self._sigmoid(eta)\n",
    "        return np.vstack([1.0 - p1, p1]).T\n",
    "\n",
    "    def predict(self, X: sp.csr_matrix) -> np.ndarray:\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(np.int32)\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, y: np.ndarray) -> Self:\n",
    "        if not sp.isspmatrix_csr(X):\n",
    "            X = sp.csr_matrix(X)\n",
    "\n",
    "        y = np.asarray(y, dtype=np.int8)\n",
    "        if set(np.unique(y)) - {0, 1}:\n",
    "            raise ValueError(\"y must be binary {0,1}.\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "        self._init_params(n_features, rng)\n",
    "\n",
    "        order = np.arange(n_samples)\n",
    "        lr = float(self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(order)\n",
    "\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for r in order:\n",
    "                start, end = X.indptr[r], X.indptr[r + 1]\n",
    "                idx = X.indices[start:end]\n",
    "                dat = X.data[start:end]\n",
    "\n",
    "                # Forward (handles empty row)\n",
    "                eta = self._decision_function_row(idx, dat)\n",
    "                p = self._sigmoid(np.array([eta]))[0]\n",
    "                e = p - y[r]  # gradient w.r.t. eta\n",
    "                total_loss += -(\n",
    "                    y[r] * np.log(p + 1e-15) + (1 - y[r]) * np.log(1 - p + 1e-15)\n",
    "                )\n",
    "\n",
    "                if self.fit_intercept:\n",
    "                    self.w0_ -= lr * e\n",
    "\n",
    "                if idx.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # w-gradient: e * x_i + l2_w * w_i\n",
    "                self.w_[idx] -= lr * (e * dat + self.l2_w * self.w_[idx])\n",
    "\n",
    "                # V-gradient (vectorized over active indices)\n",
    "                V_slice = self.V_[idx, :]  # (nnz, k)\n",
    "                p_k = V_slice.T @ dat  # (k,)\n",
    "                # grad_V_slice = e * [ x_i * (p_k - v_i * x_i) ] + l2_V * v_i\n",
    "                grad_V_slice = (\n",
    "                    e * (dat[:, None] * (p_k[None, :] - V_slice * dat[:, None]))\n",
    "                    + self.l2_V * V_slice\n",
    "                )\n",
    "                V_slice -= lr * grad_V_slice\n",
    "                self.V_[idx, :] = V_slice\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"[FM] Epoch {epoch + 1}/{self.n_epochs}, LogLoss: {total_loss / n_samples:.6f}\"\n",
    "                )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c793f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit  # Numerically stable sigmoid function\n",
    "\n",
    "\n",
    "class FactorizationMachineClassifier1(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rank: int = 16,\n",
    "        n_epochs: int = 10,\n",
    "        learning_rate: float = 0.05,\n",
    "        l2_w: float = 1e-6,\n",
    "        l2_V: float = 1e-6,\n",
    "        init_stdev: float = 0.1,\n",
    "        fit_intercept: bool = True,\n",
    "        shuffle: bool = True,\n",
    "        random_state: int = RANDOM_STATE,\n",
    "        verbose: int = 1,\n",
    "    ) -> None:\n",
    "        self.rank = rank\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_w = l2_w\n",
    "        self.l2_V = l2_V\n",
    "        self.init_stdev = init_stdev\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _init_params(self, n_features: int, rng: np.random.RandomState) -> None:\n",
    "        self.w0_ = 0.0 if self.fit_intercept else 0.0\n",
    "        self.w_ = np.zeros(n_features, dtype=np.float64)\n",
    "        self.V_ = rng.normal(\n",
    "            scale=self.init_stdev, size=(n_features, self.rank)\n",
    "        ).astype(np.float64)\n",
    "\n",
    "    def decision_function(self, X: sp.csr_matrix) -> np.ndarray:\n",
    "        check_is_fitted(self, attributes=[\"w_\", \"V_\"])\n",
    "\n",
    "        # 1. Intercept term\n",
    "        eta = self.w0_ if self.fit_intercept else 0.0\n",
    "\n",
    "        # 2. Linear term: X @ w\n",
    "        eta += X @ self.w_\n",
    "\n",
    "        # 3. Pairwise interaction term: 0.5 * sum_k( (X @ V_k)^2 - (X^2 @ V_k^2) )\n",
    "        # To avoid creating a dense matrix, this is calculated efficiently.\n",
    "        term1 = (X @ self.V_) ** 2\n",
    "\n",
    "        # Element-wise square of sparse matrix data\n",
    "        X_sq = X.copy()\n",
    "        X_sq.data **= 2\n",
    "\n",
    "        term2 = X_sq @ (self.V_**2)\n",
    "\n",
    "        eta += 0.5 * np.sum(term1 - term2, axis=1)\n",
    "\n",
    "        return eta\n",
    "\n",
    "    def predict_proba(self, X: sp.csr_matrix) -> np.ndarray:\n",
    "        eta = self.decision_function(X)\n",
    "\n",
    "        p1 = expit(eta)\n",
    "\n",
    "        return np.vstack([1.0 - p1, p1]).T\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, y: np.ndarray) -> Self:\n",
    "        # X, y = validate_data(X, y)\n",
    "\n",
    "        # Check that y is binary\n",
    "        if len(np.unique(y)) != 2:\n",
    "            raise ValueError(\"y must be binary {0,1}.\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "        self._init_params(n_features, rng)\n",
    "\n",
    "        order = np.arange(n_samples)\n",
    "        lr = float(self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(order)\n",
    "\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for r in order:\n",
    "                start, end = X.indptr[r], X.indptr[r + 1]\n",
    "                idx = X.indices[start:end]\n",
    "                dat = X.data[start:end]\n",
    "\n",
    "                # Skip empty rows\n",
    "                if idx.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # --- Forward Pass (for a single instance) ---\n",
    "                eta = self.w0_\n",
    "\n",
    "                # Linear term\n",
    "                eta += np.dot(self.w_[idx], dat)\n",
    "\n",
    "                # Interaction term (optimized calculation)\n",
    "                V_slice = self.V_[idx]\n",
    "                p_k = dat @ V_slice  # (rank,)\n",
    "                interaction_term = 0.5 * (\n",
    "                    np.sum(p_k**2) - np.sum((dat[:, None] * V_slice) ** 2)\n",
    "                )\n",
    "                eta += interaction_term\n",
    "\n",
    "                # --- Gradient Calculation ---\n",
    "                p = expit(eta)\n",
    "                e = p - y[r]  # Gradient of log-loss w.r.t. eta\n",
    "\n",
    "                # Accumulate log loss (with a small epsilon for stability)\n",
    "                total_loss += -(\n",
    "                    y[r] * np.log(p + 1e-15) + (1 - y[r]) * np.log(1 - p + 1e-15)\n",
    "                )\n",
    "\n",
    "                # --- Parameter Updates ---\n",
    "                if self.fit_intercept:\n",
    "                    self.w0_ -= lr * e\n",
    "\n",
    "                # Update w (linear terms)\n",
    "                grad_w = e * dat + self.l2_w * self.w_[idx]\n",
    "                self.w_[idx] -= lr * grad_w\n",
    "\n",
    "                # Update V (interaction terms)\n",
    "                # IMPROVEMENT: More idiomatic and potentially faster in-place update.\n",
    "                grad_V_slice = (\n",
    "                    e * (dat[:, None] * (p_k - V_slice * dat[:, None]))\n",
    "                    + self.l2_V * V_slice\n",
    "                )\n",
    "                self.V_[idx] -= lr * grad_V_slice\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"[FM] Epoch {epoch + 1}/{self.n_epochs}, \"\n",
    "                    f\"LogLoss: {total_loss / n_samples:.6f}\"\n",
    "                )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f8ce8b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int64(61384)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/click-through-learn-j_p1wCCt-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 61384",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m fm = FactorizationMachineClassifier1(\n\u001b[32m      2\u001b[39m     rank=\u001b[32m16\u001b[39m,\n\u001b[32m      3\u001b[39m     n_epochs=\u001b[32m9\u001b[39m,\n\u001b[32m      4\u001b[39m     learning_rate=\u001b[32m0.01\u001b[39m,\n\u001b[32m      5\u001b[39m     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m fm_pipeline = Pipeline(\n\u001b[32m      8\u001b[39m     steps=[\n\u001b[32m      9\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, featurizer),\n\u001b[32m     10\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m\"\u001b[39m, fm),\n\u001b[32m     11\u001b[39m     ]\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mfm_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m y_pred_proba = fm_pipeline.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     16\u001b[39m y_pred = (y_pred_proba >= \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/click-through-learn-j_p1wCCt-py3.12/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/click-through-learn-j_p1wCCt-py3.12/lib/python3.12/site-packages/sklearn/pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mFactorizationMachineClassifier1.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# --- Gradient Calculation ---\u001b[39;00m\n\u001b[32m    111\u001b[39m p = expit(eta)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m e = p - \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Gradient of log-loss w.r.t. eta\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Accumulate log loss (with a small epsilon for stability)\u001b[39;00m\n\u001b[32m    115\u001b[39m total_loss += -(\n\u001b[32m    116\u001b[39m     y[r] * np.log(p + \u001b[32m1e-15\u001b[39m) + (\u001b[32m1\u001b[39m - y[r]) * np.log(\u001b[32m1\u001b[39m - p + \u001b[32m1e-15\u001b[39m)\n\u001b[32m    117\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/click-through-learn-j_p1wCCt-py3.12/lib/python3.12/site-packages/pandas/core/series.py:1133\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/click-through-learn-j_p1wCCt-py3.12/lib/python3.12/site-packages/pandas/core/series.py:1249\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/click-through-learn-j_p1wCCt-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(61384)"
     ]
    }
   ],
   "source": [
    "fm = FactorizationMachineClassifier1(\n",
    "    rank=16,\n",
    "    n_epochs=9,\n",
    "    learning_rate=0.01,\n",
    "    verbose=1,\n",
    ")\n",
    "fm_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"features\", featurizer),\n",
    "        (\"clf\", fm),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fm_pipeline.fit(X_train, y_train)\n",
    "y_pred_proba = fm_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"LogLoss (FM):\", log_loss(y_test, y_pred_proba))\n",
    "print(\"ROC-AUC (FM):\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"PR-AUC  (FM):\", average_precision_score(y_test, y_pred_proba))\n",
    "print(\"Accuracy  (FM):\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4492c",
   "metadata": {},
   "source": [
    "#### Metrics / Plots\n",
    "\n",
    "---\n",
    "\n",
    "Use these helpers to evaluate (logloss, ROC-AUC, PR-AUC) and visualize learning curves when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fec14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install plotly\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# from sklearn.metrics import log_loss, roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "# def evaluate_scores(y_true, y_pred_proba, label: str = \"model\", show: bool = True):\n",
    "#     \"\"\"\n",
    "#     Compute LogLoss / ROC-AUC / PR-AUC and display them as a Plotly table.\n",
    "#     Accepts either a 1D array of positive-class probabilities or a 2D array with [:,1].\n",
    "#     \"\"\"\n",
    "#     y_pred = (\n",
    "#         y_pred_proba[:, 1] if getattr(y_pred_proba, \"ndim\", 1) == 2 else y_pred_proba\n",
    "#     )\n",
    "\n",
    "#     metrics = {\n",
    "#         \"Model\": [label],\n",
    "#         \"LogLoss\": [log_loss(y_true, y_pred)],\n",
    "#         \"ROC-AUC\": [roc_auc_score(y_true, y_pred)],\n",
    "#         \"PR-AUC\": [average_precision_score(y_true, y_pred)],\n",
    "#     }\n",
    "\n",
    "#     fig = go.Figure(\n",
    "#         data=[\n",
    "#             go.Table(\n",
    "#                 header=dict(\n",
    "#                     values=list(metrics.keys()), fill_color=\"#f5f5f5\", align=\"left\"\n",
    "#                 ),\n",
    "#                 cells=dict(values=list(metrics.values()), align=\"left\"),\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     fig.update_layout(\n",
    "#         title=f\"Evaluation Metrics — {label}\", margin=dict(l=10, r=10, t=40, b=10)\n",
    "#     )\n",
    "\n",
    "#     if show:\n",
    "#         print(f\"[{label}] LogLoss: {metrics['LogLoss'][0]:.6f}\")\n",
    "#         print(f\"[{label}] ROC-AUC: {metrics['ROC-AUC'][0]:.6f}\")\n",
    "#         print(f\"[{label}] PR-AUC : {metrics['PR-AUC'][0]:.6f}\")\n",
    "#         fig.show()\n",
    "\n",
    "#     return metrics, fig\n",
    "\n",
    "\n",
    "# def plot_learning_curve(xs, ys):\n",
    "#     \"\"\"\n",
    "#     Interactive learning curve with best-epoch marker.\n",
    "#     xs: list/array of epochs\n",
    "#     ys: list/array of logloss values\n",
    "#     \"\"\"\n",
    "#     xs = np.asarray(xs)\n",
    "#     ys = np.asarray(ys)\n",
    "#     best_idx = int(np.argmin(ys))\n",
    "\n",
    "#     fig = go.Figure()\n",
    "#     fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines+markers\", name=\"logloss\"))\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=[xs[best_idx]],\n",
    "#             y=[ys[best_idx]],\n",
    "#             mode=\"markers+text\",\n",
    "#             text=[f\"best={ys[best_idx]:.5f} @ {xs[best_idx]}\"],\n",
    "#             textposition=\"top center\",\n",
    "#             marker=dict(size=10, symbol=\"star\"),\n",
    "#         )\n",
    "#     )\n",
    "#     fig.update_layout(\n",
    "#         title=\"LogLoss vs Epoch\",\n",
    "#         xaxis_title=\"epoch\",\n",
    "#         yaxis_title=\"logloss\",\n",
    "#         template=\"plotly_white\",\n",
    "#         margin=dict(l=10, r=10, t=40, b=10),\n",
    "#     )\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "\n",
    "# # metrics, table_fig = evaluate_scores(y_train, y_pred_proba)\n",
    "# lc_fig = plot_learning_curve(range(1, len(10) + 1), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d84a6",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Practical tips (Avazu-scale)\n",
    "- **Split:** by day (`hour`), hold-out last day for validation.\n",
    "- **Hashing:** start with `2**22` or `2**24` features. Increase if collisions are harmful.\n",
    "- **FM rank (`k`)**: 8–32 are common for CTR. Tune with validation.\n",
    "- **Regularization:** small `l2_reg_w`, `l2_reg_V` (e.g., `1e-6..1e-3`) for FM; `C`/penalty for LR.\n",
    "- **Learning rate:** `fastFM` `step_size` ~ `1e-2..1e-1`; `xlearn` `lr` ~ `0.05..0.3`.\n",
    "- **Batching:** stream CSV in chunks; keep validation separate.\n",
    "- **FFM:** often beats FM on CTR when fields are well-defined (user/app/device/site…).  \n",
    "- **Calibration:** if you need perfectly calibrated CTRs, consider Platt/Isotonic on the validation set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "click-through-learn-j_p1wCCt-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
